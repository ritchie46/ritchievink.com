<!DOCTYPE html>
<html lang="en-EN">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta content="keyword 1, keyword 2, keyword 3" name="keywords">
<meta content="Ritchie Vink" name="author">
<meta property="og:title" content="Another normalizing flow: Inverse Autoregressive Flows - Ritchie Vink">
<meta property="og:url" content="https://www.ritchievink.com/blog/2019/11/12/another-normalizing-flow-inverse-autoregressive-flows/">
<meta property="og:description" content="">
<meta property="og:type" content="website" />


<meta property="og:image" content="https://www.ritchievink.com/img/post-30-iaf/og_image.jpg" />


<title>Another normalizing flow: Inverse Autoregressive Flows | Ritchie Vink</title>

<link rel="stylesheet" href="https://www.ritchievink.com//css/style.css">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />

<link rel="stylesheet"
      href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.0/build/styles/default.min.css">

</head>

<body>
<section class="section">
  <div class="container">
    <nav class="nav">

 <img src="../../../../../profile.jpg" alt="Avatar" style="margin-right: 1em" height="100px"> 
      <div class="nav-left" style="flex-basis: auto;">

        <a class="nav-item" href="https://www.ritchievink.com/"><h1 class="title is-4">Ritchie Vink</h1></a>
      <nav class="nav-item level is-mobile">
          <a class="level-item" href="../../../../../tags">
            tags
          </a>
          
          
          <a class="level-item" href="https://www.ritchievink.com/about/">
            about
          </a>
          
          <a class="level-item" href="https://www.ritchievink.com/anastruct/">
            anastruct
          </a>
          
        </nav>
      </div>
      <div class="nav-right">
        <nav class="nav-item level is-mobile">
          
          <a class="level-item" href="https://github.com/ritchie46" target="_blank">
            <span class="icon">
              <i class="fa fa-github"></i>
            </span>
          </a>
          
          <a class="level-item" href="https://linkedin.com/in/ritchievink/" target="_blank">
            <span class="icon">
              <i class="fa fa-linkedin-square"></i>
            </span>
          </a>
          
          <a class="level-item" href="https://www.ritchievink.com/index.xml" target="_blank">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>
          </a>
          
        </nav>
      </div>
    </nav>
  </div>
</section>

<section class="section">
  <div class="container">
    <h1 class="title">Another normalizing flow: Inverse Autoregressive Flows</h1>
    <h2 class="subtitle is-5">November 12, 2019 by Ritchie Vink</h2>
    
      <div class="tags">
    
        <a class="button is-link" href="https://www.ritchievink.com/tags/machine-learning">machine learning</a>
    
        <a class="button is-link" href="https://www.ritchievink.com/tags/python">python</a>
    
        <a class="button is-link" href="https://www.ritchievink.com/tags/deep-learning">deep-learning</a>
    
        <a class="button is-link" href="https://www.ritchievink.com/tags/bayesian">bayesian</a>
    
        <a class="button is-link" href="https://www.ritchievink.com/tags/pytorch">pytorch</a>
    
</div>

    
    <div class="content">
      <figure><img src="../../../../../img/post-30-iaf/og_image.jpg"/>
</figure>

<br>
This post we will explore a type of normalizing flow called **Inverse Autoregressive Flow**. A composition (flow) of transformations, while preserving the constraints of a probability distribution (normalizing), can help us obtain highly correlated variational distributions. 
<p><em>Don&rsquo;t repeat yourself</em><br>
If what was mentioned in the previous lines didn&rsquo;t ring a bell, do first read these posts: <a href="https://www.ritchievink.com/blog/2019/09/16/variational-inference-from-scratch/">variational inference</a> and <a href="https://www.ritchievink.com/blog/2019/10/11/sculpting-distributions-with-normalizing-flows/">normalizing flows</a>. This post could really be seen as an extension of the latter.</p>
<h2 id="1-planar-radial-flows">1. Planar/ Radial flows</h2>
<p>Rezende &amp; Mohammed<sup>[1]</sup> discussed two normalizing flows, the planar flow, and the radial flow. If we take a look at the definition of the planar flow transformation:</p>
<p>\begin{equation}
f(z) = z + u h(w^Tz + b)
\end{equation}</p>
<p>Where the latent variable $z \in \mathbb{R}^D$, and the transformations learnable parameters; $u \in \mathbb{R}^D, w \in \mathbb{R}^D, b \in \mathbb{R}$, and $h(\cdot)$ is a smooth activation function. <br>
As $w^Tz \mapsto \mathbb{R}^1$, the transformation is comparable to a neural network layer with <strong>one</strong> activation node. Evidently, this is quite restrictive and we need a lot of transformations to be able to transform high dimensional space. This same restriction is true of the radial flow variant.</p>
<h2 id="2-improving-normalizing-flows">2. Improving normalizing flows</h2>
<p>As discussed in the post <a href="https://www.ritchievink.com/blog/2019/10/11/sculpting-distributions-with-normalizing-flows/">normalizing flows</a>, a normalizing flow is defined as:</p>
<p>\begin{eqnarray}
Q(z&rsquo;) &amp;=&amp; Q(z) \left| \det \frac{\partial{f}}{\partial{z}} \right|^{-1}
\end{eqnarray}</p>
<p>Where $f(\cdot)$ is an invertible transformation $f: \mathbb{R}^m \mapsto \mathbb{R}^m$. To have a tractable distribution, the determinant Jacobian $\left| \det \frac{\partial{f}}{\partial{z}} \right|$ should be cheaply computable.</p>
<p>This tractability constraint isn&rsquo;t easily met, and therefore there is now a field of research that focusses on transformations $f(\cdot)$ that have tractable determinant Jacobians. With radial/ planar flows tractability is achieved by applying the <a href="https://en.wikipedia.org/wiki/Matrix_determinant_lemma">matrix determinant lemma</a>.</p>
<h3 id="22-autoregression">2.2 Autoregression</h3>
<p>Autoregressive functions do have tractable determinant jacobians. Let $z&rsquo;$ be our transformed random variable $f(z) = z&rsquo;$. An autoregressive function would transform every dimension $i$ by:</p>
<p>\begin{eqnarray}
z&rsquo;_i = f(z_{1:i})
\end{eqnarray}</p>
<p>The Jacobian matrix $J = \frac{\partial{z&rsquo;}}{\partial{z}}$ will then be lower triangular. Which is cheaply computed by the product of the diagonal values:</p>
<p>\begin{eqnarray}
\det J = \prod_{i=1}^D J_{ii}
\end{eqnarray}</p>
<p>Tractable determinant Jacobians are thus possible by using autoregressive transformations. The first time I read <em>this paper</em><sup>[3]</sup>, I wondered how we could ensure a neural network would be an autoregressive transformation without using RNNs. It turns out you can mask the connection of an autoencoder in such a way that that the output $x_i$ is only connected to the previous inputs $x_{i:k}$. This was shown in the MADE<sup>[4]</sup> paper, which was the topic of my previous post <a href="https://www.ritchievink.com/blog/2019/10/25/distribution-estimation-with-masked-autoencoders/">Distribution estimation with Masked Autoencoders</a>.</p>
<p><em>Note: In the MADE post we modeled $x_i = f(x_{1:i-1})$, which would lead to a lower triangular Jacobian with zeros on the diagonal, and thus leading to $\det J = 0$. Later in this post we&rsquo;ll see that this won&rsquo;t be a problem.</em></p>
<h3 id="23-autoregressive-transformation">2.3 Autoregressive transformation</h3>
<p>The radial/ planar transformations are effectively a single layer with one activation node. The relations between different dimensions of $z$ can therefore not be very complex. With autoregressive transformations we can model more complexity as $z_i$ is dependent on $z_{1:i-1}$, and thus connected to all lower dimensions of $i$. Let&rsquo;s define such a transformation. Let $z&rsquo; = f(z)$, and the first dimensions of the transformed variable be defined by:</p>
<p>\begin{eqnarray}
z &amp;\sim&amp; \mathcal{N}(0, I) &amp;\qquad \tiny{\text{base distribution: } Q(z) } \\
z&rsquo;_0 &amp;=&amp; \mu_0 + \sigma_0 \odot z_0 \label{art0}
\end{eqnarray}</p>
<p>Where $\epsilon \in \mathbb{R}^D$. Effectively meaning that the first dimension of the transformation is randomly sampled. The other dimensions $i \gt 0$ are computed by:</p>
<div>
\begin{eqnarray}
z'_i = \mu_i(z'_{1:i-1}) + \sigma_i(z'_{1:i-1}) \cdot z_i  \label{art}
\end{eqnarray}
</div>
<p>Eq. \eqref{art} clearly depicts the downside of this transformation. Applying the transformation has a complexity of $\mathcal{O}(D)$ (without any possibility to parallelize). Which can become rather expensive as we will apply a flow of $k$ transformations. Actually sampling from this distribution would be proportional to $D \times k$ operations.</p>
<h3 id="24-inverse-autoregressive-transformation">2.4 Inverse Autoregressive Transformation</h3>
<p>We can inverse the operations defined in eq. \eqref{art0} and eq. \eqref{art}.</p>
<div>
\begin{eqnarray}
z_0 &=& \frac{z'_0 - \mu_0}{\sigma_0} \label{iart0} \\
z_i &=& \frac{z'_i - \mu(z'_{1:i-1}) }{\sigma(z'_{1:i-1})} \label{iart}
\end{eqnarray}
</div>
<p><small><em>Note that due to the inversion $z&rsquo; = f(z)$ now is $z = f(z&rsquo;)$!</em></small></p>
<p>The inverse is only dependant on $z&rsquo;$ (in this case, the value <strong>before</strong> the transformation), and can thus easily be parallelized. Besides cheap sampling, we also need to derive the determinant Jacobian. The Jacobian lower triangular, which means we only need to compute the partial derivatives of the diagonal to compute the determinant.</p>
<div>
\begin{equation}
\frac{d{\bf z}}{d{\bf z'}} =
\begin{bmatrix}
\frac{z_0}{z'_0 } & 0 & \dots & 0 \\
\frac{z_0}{z'_1 } & \frac{z_1}{z'_1} & \dots  & 0 \\
\vdots & \ddots & \ddots & \vdots \\
\frac{z_0}{z'_D } & \dots & \frac{\partial z_{D-1}}{\partial z'_D} &  \frac{\partial z_{D}}{\partial z'_D} \\
\end{bmatrix}
\end{equation}
</div>
<p>The diagonal is <span>$\{{ \frac{z_0}{z&rsquo;_0}, \frac{z_1}{z&rsquo;_1}, \dots, \frac{z_D}{z&rsquo;_D }   \}}$</span>.</p>
<p>As $\mu(\cdot)$ and $\sigma(\cdot)$ are <strong>not dependant of $z&rsquo;_i$ (but only on $z_{1:i-1}$)</strong>, the partial derivatives evaluate to <span> $ \{{  \frac{1}{\sigma_0}, \frac{1}{\sigma_1(z&rsquo;_1)}, \dots, \frac{1} {\sigma_D(z&rsquo;_{1:D-1}) } \}}$ </span>, and thus we also have a tractable determinant Jacobian;</p>
<div>
\begin{eqnarray}
\det \left| \frac{d{\bf z}}{d{\bf z'}} \right| &=& \prod_{i=1}^D  \frac{1}{\sigma_i(z_{1:i-1})} \\
\log \det \left| \frac{d{\bf z}}{d{\bf z'}} \right| &=& \sum_{i=1}^D - \log \sigma_i(z_{1:i-1}) \\
\end{eqnarray}
</div>
<h2 id="3-inverse-autoregressive-flow">3. Inverse Autoregressive Flow</h2>
<p>That wraps all the ingredients we need to build a tractable flow. We will use the inverse functions defined in eq. $\eqref{iart0}$ and eq. $\eqref{iart}$. Let <strong>$t$</strong> be one of <strong>$k$</strong> flows. Instead of modeling $\mu_t(\cdot)$ and $\sigma_t(\cdot)$ directly, we will define two functions ${\bf s}_t = \frac{1}{\sigma_t(\cdot)}$, and ${\bf m}_t = \frac{-\mu_t(\cdot)}{\sigma_t(\cdot)}$. These will be modeled by <strong>autoregressive neural networks</strong>.</p>
<div>
\begin{eqnarray}
z_t &=& \frac{z_{t-1} - \mu_t(z_{t-1}) }{\sigma_t(z_{t-1})} \\
    &=& \frac{z_{t-1}}{\sigma_t(z_{t-1})} - \frac{\mu_t(z_{t-1})) }{\sigma_t(z_{t-1}) } \\
    &=& z_{t-1} \odot {\bf s}_t(z_{t-1})  + {\bf m}_t (z_{t-1}) \label{upd}
\end{eqnarray}
</div>
<h3 id="32-hidden-context-and-numerical-stability">3.2 Hidden context and numerical stability</h3>
<p>The authors<sup>[3]</sup> also discuss a hidden input $h$ for the autoregressive models, $[{\bf s}_t, {\bf m}_t ]$.</p>
<p>\begin{equation}
[{\bf s}_t, {\bf m}_t ] \leftarrow \text{autoregressiveNN}[t]({\bf z}_t,{\bf h}; {\bf \theta})
\end{equation}</p>
<p>And finally for numerical stability they modify eq. \eqref{upd}, by an update rule inspired by LSTM gates.</p>
<div>
\begin{eqnarray}
g_t &=& \text{sigmoid}({\bf s}_t(z_{t-1}))  \\
z_t &=& z_{t-1} \odot g_t  + (1 - g_t) \odot {\bf m}_t (z_{t-1}) \label{transf}
\end{eqnarray}
</div>
<p><small><em>Note that I deliberately chose $g_t$ instead of $\sigma_t$, as the authors do, so that it can&rsquo;t be confused with the $\sigma(\cdot)$ defined earlier.</em></small></p>
<p>Let&rsquo;s convince ourselves that this modification of eq. \eqref{upd} is allowed. It doesn&rsquo;t break the autoregressive nature of the transformation. Therefore the Jacobian $\frac{d{\bf z_t}}{d{\bf z_{t-1}}}$ is still lower triangular. The determinant Jacobian is defined by the product of the diagonal Jacobian:</p>
<div>
\begin{eqnarray}
\det \left| \frac{d{\bf z_t}}{d{\bf z_{t-1}}} \right| &=& \prod_{i=1}^D g_{t,i} \\
\log \det \left| \frac{d{\bf z_t}}{d{\bf z_{t-1}}} \right| &=& \sum_{i=1}^D g_{t,i} \label{logdet}
\end{eqnarray}
</div>
<p><small><em>Note that ${\bf m}_t$ is not dependent of $z_{t-1,i}$ only on $z_{t,1:i-1}$ and therefore is not a term in the partial derivative.</em></small></p>
<p>Now we really have defined everything needed for variational inference. Eq. \eqref{transf} will be the transformation we apply, and eq. \eqref{logdet} is the final <strong>log determinant Jacobian</strong> used in the <strong>variational free energy</strong> $\mathcal{F(x)} $;</p>
<div>
\begin{eqnarray}
\mathcal{F(x)} =  E_{z \sim Q}[\log Q(z_0) - \sum_{k=1}^{K} \log \left| \det \frac{\partial{z_t}}{\partial{z_{t-1}}} \right| - \log P(x, z_K)] \label{vfeflow} \\
\end{eqnarray}
</div>
<p>For a derivation of the variational free energy see <a href="https://www.ritchievink.com/blog/2019/10/11/sculpting-distributions-with-normalizing-flows/">the normalizing flows post</a>.</p>
<h2 id="pytorch-implementation--results">Pytorch implementation &amp; results</h2>
<p>Below I&rsquo;ve shown a code snippet of a working implementation in Pytorch. The full working exhample is hosted on <a href="https://github.com/ritchie46/vi-torch/tree/f603a53f5700a38c872a316e182d9fcffe5c5b46">github</a>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AutoRegressiveNN</span>(MADE):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, in_features, hidden_features, context_features):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__(in_features, hidden_features)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>context <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(context_features, in_features)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># remove MADE output layer</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">del</span> self<span style="color:#f92672">.</span>layers[len(self<span style="color:#f92672">.</span>layers) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, z, h):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>layers(z) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>context(h)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">IAF</span>(KL_Layer):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, size<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, context_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, auto_regressive_hidden<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>context_size <span style="color:#f92672">=</span> context_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>s_t <span style="color:#f92672">=</span> AutoRegressiveNN(
</span></span><span style="display:flex;"><span>            in_features<span style="color:#f92672">=</span>size,
</span></span><span style="display:flex;"><span>            hidden_features<span style="color:#f92672">=</span>auto_regressive_hidden,
</span></span><span style="display:flex;"><span>            context_features<span style="color:#f92672">=</span>context_size,
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>m_t <span style="color:#f92672">=</span> AutoRegressiveNN(
</span></span><span style="display:flex;"><span>            in_features<span style="color:#f92672">=</span>size,
</span></span><span style="display:flex;"><span>            hidden_features<span style="color:#f92672">=</span>auto_regressive_hidden,
</span></span><span style="display:flex;"><span>            context_features<span style="color:#f92672">=</span>context_size,
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">determine_log_det_jac</span>(self, g_t):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>log(g_t <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-6</span>)<span style="color:#f92672">.</span>sum(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, z, h<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> h <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            h <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(self<span style="color:#f92672">.</span>context_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Initially s_t should be large, i.e. 1 or 2.</span>
</span></span><span style="display:flex;"><span>        s_t <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>s_t(z, h) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1.5</span>
</span></span><span style="display:flex;"><span>        g_t <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>sigmoid(s_t)
</span></span><span style="display:flex;"><span>        m_t <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>m_t(z, h)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># log |det Jac|</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>_kl_divergence_ <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>determine_log_det_jac(g_t)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># transformation</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> g_t <span style="color:#f92672">*</span> z <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> g_t) <span style="color:#f92672">*</span> m_t
</span></span></code></pre></div><p>I&rsquo;ve trained a normalizing flow on a target distribution in 2D. The base distribution was a factorized unit Gaussian $\mathcal{N}(0, I)$. This distribution was transformed 4 times, as the examples were run with 4 flow layers. The prior distribution was also is uniform over the whole domain.</p>
<p>The first figure shows the Inverse Autoregressive flow. The second figure shows the Planar flow.</p>
<figure><img src="../../../../../img/post-30-iaf/flow1.png"/><figcaption>
            <h4>4 IAF layers</h4>
        </figcaption>
</figure>

<figure><img src="../../../../../img/post-30-iaf/flow2.png"/><figcaption>
            <h4>4 Planar layers</h4>
        </figcaption>
</figure>

<p>We see that the IAF layers are able to model the posterior $P(Z|X)$ better as well as stay closer to the prior $P(Z)$. The base distribution is less flattened and is a bit more uniform (though not on the whole domain). By observing that the base distribution is less modified we can conclude that normalizing flow is more powerful and is better able to morph the base distribution in the required density.</p>
<p>This is also visible in the figure below. This is a result from the original authors. They trained a VAE on a toy dataset with 4 data points. What we see is that the  VAE with IAF layers has a base distribution that is more true to a unit Gaussian.</p>
<figure><img src="../../../../../img/post-30-iaf/priorfit.png"/><figcaption>
            <h4>Prior and Base distributions. $^{[3]}$</h4>
        </figcaption>
</figure>

<h2 id="conclusion">Conclusion</h2>
<p>And that&rsquo;s a wrap! Another post on normalizing flows. This field of research seems really promising. But as we have seen this post, coming up with new normalizing flows isn&rsquo;t easy. Here the authors were able to do so because they made two clever observations;</p>
<ol>
<li>Autoregressive transformation lead to simple determinant Jacobians i.e. invertible transformations</li>
<li>The inverse of an autoregressive operation isn&rsquo;t expensive in sampling.</li>
</ol>
<p>If you want to read more about different types of normalizing flows out there, I&rsquo;d recommend taking a look at this <a href="http://akosiorek.github.io/ml/2018/04/03/norm_flows.html">blog post</a>.</p>
<h2 id="references">References</h2>
<p>  [1] Rezende &amp; Mohammed (2016, Jun 14) <em>Variational Inference with Normalizing Flows</em>. Retrieved from <a href="https://arxiv.org/pdf/1505.05770.pdf">https://arxiv.org/pdf/1505.05770.pdf</a> <br>
  [2] Adam Kosiorek (2018, Apr 3) <em>Normalizing Flows</em>. Retrieved from <a href="http://akosiorek.github.io/ml/2018/04/03/norm_flows.html">http://akosiorek.github.io/ml/2018/04/03/norm_flows.html</a> <br>
  [3] Kingma, Salimans, Jozefowicz, Chen, Sutskever, &amp; Welling (2016, Jun 15) <em>Improving Variational Inference with Inverse Autoregressive Flow</em>. Retrieved from <a href="https://arxiv.org/abs/1606.04934">https://arxiv.org/abs/1606.04934</a> <br>
  [4] Germain, Gregor &amp; Larochelle (2015, Feb 12) <em>MADE: Masked Autoencoder for Distribution Estimation</em>. Retrieved from <a href="https://arxiv.org/abs/1502.03509">https://arxiv.org/abs/1502.03509</a> <br></p>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
  </script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<head>
<style>

.formula-wrap {
overflow-x: scroll;
}

</style>
</head>

    </div>
    
    
  </div>
</section>

<section class="section">
  <div class="container">
    <aside><div id="disqus_thread"></div></aside>
    <script type="text/javascript">
      var disqus_shortname = 'www-ritchievink-com';
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
  </div>
</section>


<section class="section">
  <div class="container has-text-centered">
    <p>(c) 2020 Ritchie Vink.</p>
  </div>
</section>

<script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.0/build/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/javascript">
    if (window.location.href.indexOf('localhost') < 0) {
	    var _gaq = _gaq || [];
	    _gaq.push(['_setAccount', 'UA-83196691-2']);
	    _gaq.push(['_trackPageview']);

	    (function() {
		var ga = document.createElement('script');
		ga.src = ('https:' == document.location.protocol ? 'https://ssl' :
		    'http://www') + '.google-analytics.com/ga.js';
		ga.setAttribute('async', 'true');
		document.documentElement.firstChild.appendChild(ga);
	    })();
}
</script>




</body>

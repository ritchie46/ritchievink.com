<!DOCTYPE html>
<html lang="en-EN">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta content="keyword 1, keyword 2, keyword 3" name="keywords">
<meta content="Ritchie Vink" name="author">
<meta property="og:title" content="An intuitive introduction to Gaussian processes - Ritchie Vink">
<meta property="og:url" content="https://www.ritchievink.com/blog/2019/02/01/an-intuitive-introduction-to-gaussian-processes/">
<meta property="og:description" content="">
<meta property="og:type" content="website" />


<meta property="og:image" content="https://www.ritchievink.com/img/post-21-gp/waves-banner.jpg" />


<title>An intuitive introduction to Gaussian processes | Ritchie Vink</title>

<link rel="stylesheet" href="https://www.ritchievink.com//css/style.css">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />

<link rel="stylesheet"
      href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.0/build/styles/default.min.css">

</head>

<body>
<section class="section">
  <div class="container">
    <nav class="nav">

 <img src="../../../../../profile.jpg" alt="Avatar" style="margin-right: 1em" height="100px"> 
      <div class="nav-left" style="flex-basis: auto;">

        <a class="nav-item" href="https://www.ritchievink.com/"><h1 class="title is-4">Ritchie Vink</h1></a>
      <nav class="nav-item level is-mobile">
          <a class="level-item" href="../../../../../tags">
            tags
          </a>
          
          
          <a class="level-item" href="https://www.ritchievink.com/about/">
            about
          </a>
          
          <a class="level-item" href="https://www.ritchievink.com/anastruct/">
            anastruct
          </a>
          
        </nav>
      </div>
      <div class="nav-right">
        <nav class="nav-item level is-mobile">
          
          <a class="level-item" href="https://github.com/ritchie46" target="_blank">
            <span class="icon">
              <i class="fa fa-github"></i>
            </span>
          </a>
          
          <a class="level-item" href="https://linkedin.com/in/ritchievink/" target="_blank">
            <span class="icon">
              <i class="fa fa-linkedin-square"></i>
            </span>
          </a>
          
          <a class="level-item" href="https://www.ritchievink.com/index.xml" target="_blank">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>
          </a>
          
        </nav>
      </div>
    </nav>
  </div>
</section>

<section class="section">
  <div class="container">
    <h1 class="title">An intuitive introduction to Gaussian processes</h1>
    <h2 class="subtitle is-5">February 1, 2019 by Ritchie Vink</h2>
    
      <div class="tags">
    
        <a class="button is-link" href="https://www.ritchievink.com/tags/algorithm-breakdown">algorithm breakdown</a>
    
        <a class="button is-link" href="https://www.ritchievink.com/tags/machine-learning">machine learning</a>
    
        <a class="button is-link" href="https://www.ritchievink.com/tags/python">python</a>
    
        <a class="button is-link" href="https://www.ritchievink.com/tags/gaussian-processes">gaussian processes</a>
    
        <a class="button is-link" href="https://www.ritchievink.com/tags/bayesian">bayesian</a>
    
</div>

    
    <div class="content">
      <figure><img src="../../../../../img/post-21-gp/waves-banner.jpg"/>
</figure>

<p>Christopher Fonnesbeck did a talk about <a href="https://youtu.be/-sIOMs4MSuA">Bayesian Non-parametric Models for Data Science using PyMC3 on PyCon 2018</a>. In this talk, he glanced over Bayes&rsquo; modeling, the neat properties of Gaussian distributions and then quickly turned to the application of Gaussian Processes, a distribution over infinite functions. Wait, but what?! How does a Gaussian represent a function? I did not understand how, but the promise of what these <strong>Gaussian Processes representing a distribution over nonlinear and nonparametric
functions</strong> really intrigued me and therefore turned into a new subject for a post. This post we&rsquo;ll go, a bit slower than Christopher did, through what Gaussian Processes are.</p>
<p>In the first part of this post we&rsquo;ll glance over some properties of multivariate Gaussian distributions, then we&rsquo;ll examine how we can use these distributions to express our expected function values and then we&rsquo;ll combine both to find a posterior distribution for Gaussian processes.</p>
<h2 id="1-introduction-the-good-old-gaussian-distribution">1. Introduction: The good old Gaussian distribution.</h2>
<p>The star of every statistics 101 college, also shines in this post because of its handy properties. Let&rsquo;s walk through some of those properties to get a feel for them.</p>
<p>A Gaussian is defined by two parameters, the mean $\mu$, and the standard deviation $\sigma$. $\mu$ expresses our expectation of $x$ and $\sigma$ our uncertainty of this expectation.</p>
<p>$$ x \sim \mathcal{N}(\mu, \sigma)$$</p>
<p>A multivariate Gaussian is parameterized by a generalization of $\mu$ and $\sigma$ to vector space. The expected value, i.e. the mean, is now represented by a vector $\vec{\mu}$. The uncertainty is parameterized by a covariance matrix $\Sigma$.</p>
<p>$$ x \sim \mathcal{N}(\mu, \Sigma) $$</p>
<p>The covariance matrix is actually a sort of lookup table, where every column and row represent a dimension, and the values are the correlation between the samples of that dimension. For this reason, it is symmetrical. As the correlation between dimension i and j is equal to the correlation between dimensions j and i.</p>
<p>$$\Sigma_{ij} = \Sigma_{ji}$$</p>
<p>Below I have plotted the Gaussian distribution belonging $\mu = [0, 0]$, and $\Sigma = \begin{bmatrix} 1 &amp;&amp; 0.6 \\ 0.6 &amp;&amp; 1 \end{bmatrix}$.</p>
<figure><img src="../../../../../img/post-21-gp/mvgs.png"/>
</figure>

<h3 id="gaussians-make-baby-gaussians">Gaussians make baby Gaussians</h3>
<p>Gaussian processes are based on Bayesian statistics, which requires you to compute the conditional and the marginal probability. Now with Gaussian distributions, both result in Gaussian distributions in lower dimensions.</p>
<h4 id="marginal-probability">Marginal probability</h4>
<p>The marginal probability of a multivariate Gaussian is really easy. Officially it is defined by the integral over the dimension we want to marginalize over.</p>
<p>Given this jointly distribution:</p>
<div class="formula-wrap">
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>p</mi>
  <mo stretchy="false">(</mo>
  <mi>x</mi>
  <mo>,</mo>
  <mi>y</mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mrow class="MJX-TeXAtom-ORD">
    <mi class="MJX-tex-caligraphic" mathvariant="script">N</mi>
  </mrow>
  <mrow>
    <mo>(</mo>
    <mtable rowspacing="4pt" columnspacing="1em">
      <mtr>
        <mtd>
          <mrow>
            <mo>[</mo>
            <mtable rowspacing="4pt" columnspacing="1em">
              <mtr>
                <mtd>
                  <msub>
                    <mi>&#x03BC;<!-- μ --></mi>
                    <mi>x</mi>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>&#x03BC;<!-- μ --></mi>
                    <mi>y</mi>
                  </msub>
                </mtd>
              </mtr>
            </mtable>
            <mo>]</mo>
          </mrow>
          <mo>,</mo>
          <mrow>
            <mo>[</mo>
            <mtable rowspacing="4pt" columnspacing="1em">
              <mtr>
                <mtd>
                  <msub>
                    <mi mathvariant="normal">&#x03A3;<!-- Σ --></mi>
                    <mi>x</mi>
                  </msub>
                </mtd>
                <mtd>
                  <msub>
                    <mi mathvariant="normal">&#x03A3;<!-- Σ --></mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>x</mi>
                      <mi>y</mi>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msubsup>
                    <mi mathvariant="normal">&#x03A3;<!-- Σ --></mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>x</mi>
                      <mi>y</mi>
                    </mrow>
                    <mi>T</mi>
                  </msubsup>
                </mtd>
                <mtd>
                  <msub>
                    <mi mathvariant="normal">&#x03A3;<!-- Σ --></mi>
                    <mi>y</mi>
                  </msub>
                </mtd>
              </mtr>
            </mtable>
            <mo>]</mo>
          </mrow>
        </mtd>
      </mtr>
    </mtable>
    <mo>)</mo>
  </mrow>
</math>
</div>
<p>The marginal distribution can be acquired by just reparameterizing the lower dimensional Gaussian distribution with $\mu_x$ and $\Sigma_x$, where normally we would need to do an integral over all possible values of $y$.</p>
<p>$$p(x) = \int{p(x, y)dy} = \mathcal{N}(\mu_x, \Sigma_x)$$</p>
<p>Below we see how integrating, (summing all the dots) leads to a lower dimensional distribution which is also Gaussian.</p>
<figure><img src="../../../../../img/post-21-gp/marginal.png" width="600px"/><figcaption>
            <h4>Marginal probability of a Gaussian distribution</h4>
        </figcaption>
</figure>

<h4 id="conditional-probability">Conditional probability</h4>
<p>The conditional probability also leads to a lower dimensional Gaussian distribution.</p>
<div class="formula-wrap">
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>p</mi>
  <mo stretchy="false">(</mo>
  <mi>x</mi>
  <mrow class="MJX-TeXAtom-ORD">
    <mo stretchy="false">|</mo>
  </mrow>
  <mi>y</mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mrow class="MJX-TeXAtom-ORD">
    <mi class="MJX-tex-caligraphic" mathvariant="script">N</mi>
  </mrow>
  <mo stretchy="false">(</mo>
  <munder>
    <mrow class="MJX-TeXAtom-OP MJX-fixedlimits">
      <munder>
        <mrow>
          <msub>
            <mi>&#x03BC;<!-- μ --></mi>
            <mi>x</mi>
          </msub>
          <mo>+</mo>
          <msub>
            <mi mathvariant="normal">&#x03A3;<!-- Σ --></mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>x</mi>
              <mi>y</mi>
            </mrow>
          </msub>
          <msubsup>
            <mi mathvariant="normal">&#x03A3;<!-- Σ --></mi>
            <mi>y</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mo>&#x2212;<!-- − --></mo>
              <mn>1</mn>
            </mrow>
          </msubsup>
          <mo stretchy="false">(</mo>
          <mi>y</mi>
          <mo>&#x2212;<!-- − --></mo>
          <msub>
            <mi>&#x03BC;<!-- μ --></mi>
            <mi>y</mi>
          </msub>
          <mo stretchy="false">)</mo>
        </mrow>
        <mo>&#x23DF;<!-- ⏟ --></mo>
      </munder>
    </mrow>
    <mrow class="MJX-TeXAtom-ORD">
      <mtext>conditional mean</mtext>
    </mrow>
  </munder>
  <mo>,</mo>
  <munder>
    <mrow class="MJX-TeXAtom-OP MJX-fixedlimits">
      <munder>
        <mrow>
          <msub>
            <mi mathvariant="normal">&#x03A3;<!-- Σ --></mi>
            <mi>x</mi>
          </msub>
          <mo>&#x2212;<!-- − --></mo>
          <msub>
            <mi mathvariant="normal">&#x03A3;<!-- Σ --></mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>x</mi>
              <mi>y</mi>
            </mrow>
          </msub>
          <msubsup>
            <mi mathvariant="normal">&#x03A3;<!-- Σ --></mi>
            <mi>y</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mo>&#x2212;<!-- − --></mo>
              <mn>1</mn>
            </mrow>
          </msubsup>
          <msubsup>
            <mi mathvariant="normal">&#x03A3;<!-- Σ --></mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>x</mi>
              <mi>y</mi>
            </mrow>
            <mi>T</mi>
          </msubsup>
        </mrow>
        <mo>&#x23DF;<!-- ⏟ --></mo>
      </munder>
    </mrow>
    <mrow class="MJX-TeXAtom-ORD">
      <mtext>conditional covariance</mtext>
    </mrow>
  </munder>
  <mo stretchy="false">)</mo>
</math>
</div>
<p>Below is shown a plot of how the conditional distribution also leads to a Gaussian distribution (in red).</p>
<figure><img src="../../../../../img/post-21-gp/cond.png" width="600px"/><figcaption>
            <h4>Conditional probability of a Gaussian distribution</h4>
        </figcaption>
</figure>

<h2 id="2-functions-described-as-multivariate-gaussians">2. Functions described as multivariate Gaussians</h2>
<p>A function $f$, is something that maps a specific set (the domain) $X$ to another set (the codomain) $Y$.</p>
<p>$$ f(x) = y$$</p>
<p>The domain and the codomain can have an infinite number of values. But let&rsquo;s imagine for now that the domain is finite and is defined by a set $X =$ {$  x_1, x_2, \ldots, x_n$}.</p>
<p>We could define a multivariate Gaussian for all possible values of $f(x)$ where $x \in X$.</p>
<div class="formula-wrap">
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
    <mo>(</mo>
    <mtable rowspacing="4pt" columnspacing="1em">
      <mtr>
        <mtd>
          <mi>f</mi>
          <mo stretchy="false">(</mo>
          <msub>
            <mi>x</mi>
            <mn>1</mn>
          </msub>
          <mo stretchy="false">)</mo>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mi>f</mi>
          <mo stretchy="false">(</mo>
          <msub>
            <mi>x</mi>
            <mn>2</mn>
          </msub>
          <mo stretchy="false">)</mo>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mo>&#x22EE;<!-- ⋮ --></mo>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mi>f</mi>
          <mo stretchy="false">(</mo>
          <msub>
            <mi>x</mi>
            <mi>n</mi>
          </msub>
          <mo stretchy="false">)</mo>
        </mtd>
      </mtr>
    </mtable>
    <mo>)</mo>
  </mrow>
  <mo>&#x223C;<!-- ∼ --></mo>
  <mrow class="MJX-TeXAtom-ORD">
    <mi class="MJX-tex-caligraphic" mathvariant="script">N</mi>
  </mrow>
  <mrow>
    <mo>(</mo>
    <mtable rowspacing="4pt" columnspacing="1em">
      <mtr>
        <mtd>
          <mrow>
            <mo>(</mo>
            <mtable rowspacing="4pt" columnspacing="1em">
              <mtr>
                <mtd>
                  <msub>
                    <mi>&#x03BC;<!-- μ --></mi>
                    <mn>1</mn>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>&#x03BC;<!-- μ --></mi>
                    <mn>2</mn>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mo>&#x22EE;<!-- ⋮ --></mo>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>&#x03BC;<!-- μ --></mi>
                    <mi>n</mi>
                  </msub>
                </mtd>
              </mtr>
            </mtable>
            <mo>)</mo>
          </mrow>
          <mo>,</mo>
          <mrow>
            <mo>(</mo>
            <mtable rowspacing="4pt" columnspacing="1em">
              <mtr>
                <mtd>
                  <msub>
                    <mi>&#x03C3;<!-- σ --></mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>11</mn>
                    </mrow>
                  </msub>
                </mtd>
                <mtd>
                  <msub>
                    <mi>&#x03C3;<!-- σ --></mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>12</mn>
                    </mrow>
                  </msub>
                </mtd>
                <mtd>
                  <mo>&#x22EF;<!-- ⋯ --></mo>
                </mtd>
                <mtd>
                  <msub>
                    <mi>&#x03C3;<!-- σ --></mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>1</mn>
                      <mi>n</mi>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>&#x03C3;<!-- σ --></mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>21</mn>
                    </mrow>
                  </msub>
                </mtd>
                <mtd>
                  <msub>
                    <mi>&#x03C3;<!-- σ --></mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>22</mn>
                    </mrow>
                  </msub>
                </mtd>
                <mtd>
                  <mo>&#x22EF;<!-- ⋯ --></mo>
                </mtd>
                <mtd>
                  <msub>
                    <mi>&#x03C3;<!-- σ --></mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mn>2</mn>
                      <mi>n</mi>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mo>&#x22EE;<!-- ⋮ --></mo>
                </mtd>
                <mtd>
                  <mo>&#x22EE;<!-- ⋮ --></mo>
                </mtd>
                <mtd>
                  <mo>&#x22F1;<!-- ⋱ --></mo>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>&#x03C3;<!-- σ --></mi>
                    <mi>n</mi>
                  </msub>
                </mtd>
                <mtd>
                  <msub>
                    <mi>&#x03C3;<!-- σ --></mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>n</mi>
                      <mn>2</mn>
                    </mrow>
                  </msub>
                </mtd>
                <mtd />
                <mtd>
                  <msub>
                    <mi>&#x03C3;<!-- σ --></mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>n</mi>
                      <mi>n</mi>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
            </mtable>
            <mo>)</mo>
          </mrow>
        </mtd>
      </mtr>
    </mtable>
    <mo>)</mo>
  </mrow>
</math>
</div>
<p>Here the $\mu$ vector contains the expected values for $f(x)$. If we are certain about the result of a function, we would say that $f(x) \approx y$ and that the $\sigma$ values would all be close to zero. Each time we sample from this distribution we&rsquo;ll get a function close to $f$. It is important to note that <strong>each finite value of x is another dimension</strong> in the multivariate Gaussian.</p>
<p>Let&rsquo;s give an example in Python.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> scipy <span style="color:#f92672">import</span> stats
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>n <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">10</span>, n)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define the gaussian with mu = sin(x) and negligible covariance matrix</span>
</span></span><span style="display:flex;"><span>norm <span style="color:#f92672">=</span> stats<span style="color:#f92672">.</span>multivariate_normal(mean<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>sin(x), cov<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>eye(n) <span style="color:#f92672">*</span> <span style="color:#ae81ff">1e-6</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">6</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Taking a sample from the distribution and plotting it.</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(x, norm<span style="color:#f92672">.</span>rvs())
</span></span></code></pre></div><figure><img src="../../../../../img/post-21-gp/sing.png"/><figcaption>
            <h4>A function sampled from a Gaussian distribution.</h4>
        </figcaption>
</figure>

<p>We can also define a distribution of functions with $\vec{\mu} = 0$ and $\Sigma = I$ (the identity matrix). Now we do have some uncertainty because the diagonal of $\Sigma$ has a standard deviation of 1. A second thing to note is that all values of $f(x)$ are completely unrelated to each other, because the correlation between all dimensions is zero. In the example below, we draw 3 functions from this distribution.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Define the gaussian with mu = 0 and a covariance matrix w/o any correlation,</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># but w/ uncertainty</span>
</span></span><span style="display:flex;"><span>norm <span style="color:#f92672">=</span> stats<span style="color:#f92672">.</span>multivariate_normal(mean<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>zeros(n), cov<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>eye(n))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">6</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Taking 3 sample from the distribution and plotting it.</span>
</span></span><span style="display:flex;"><span>[plt<span style="color:#f92672">.</span>plot(x, norm<span style="color:#f92672">.</span>rvs()) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">3</span>)]
</span></span></code></pre></div><figure><img src="../../../../../img/post-21-gp/draw3.png"/><figcaption>
            <h4>Three functions sampled from $\mathcal{N}(\vec{0}, I)$ </h4>
        </figcaption>
</figure>

<h2 id="3-controlling-the-functions-with-kernels">3. Controlling the functions with kernels</h2>
<p>As you can see we&rsquo;ve sampled different functions from our multivariate Gaussian. In fact, we can sample an infinite amount of functions from this distribution. And if we would want a more fine grid of values, we could also reparameterize our Gaussian to include a new set of $X$.</p>
<p>However, these functions we sample now are pretty random and maybe don&rsquo;t seem likely for some real-world processes. Let&rsquo;s say we only want to sample functions that are <strong>smooth</strong>. Besides that smoothness looks very slick, it is also a reasonable assumption. Values that are close to each other in domain $X$, will also be mapped close to each other in the codomain $Y$. We could construct such functions by defining the covariance matrix $\Sigma$ in such a way that values close to
each other have larger correlation than values with a larger distance between them.</p>
<h3 id="squared-exponential-kernel">Squared exponential kernel</h3>
<p>A way to create this new covariance matrix is by using a <strong>squared exponential kernel</strong>. This kernel does nothing more than assigning high correlation values to $x$ values closely together.</p>
<p>$$k(x, x&rsquo;) = exp(- \frac{(x-x&rsquo;)^2}{2l^2})$$</p>
<p>If we now define a covariance matrix $\Sigma = k(x, x)$, we sample much smoother functions.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">kernel</span>(m1, m2, l<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> l<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>) <span style="color:#f92672">*</span> (m1[:, <span style="color:#66d9ef">None</span>] <span style="color:#f92672">-</span> m2)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>n <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">10</span>, n)
</span></span><span style="display:flex;"><span>cov <span style="color:#f92672">=</span> kernel(x, x, <span style="color:#ae81ff">0.44</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>norm <span style="color:#f92672">=</span> stats<span style="color:#f92672">.</span>multivariate_normal(mean<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>zeros(n), cov<span style="color:#f92672">=</span>cov)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">6</span>))
</span></span><span style="display:flex;"><span>[plt<span style="color:#f92672">.</span>plot(x, norm<span style="color:#f92672">.</span>rvs()) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">3</span>)]
</span></span></code></pre></div><figure><img src="../../../../../img/post-21-gp/smoes.png"/><figcaption>
            <h4>Three functions sampled from $\mathcal{N}(\vec{0}, k(x, x))$ </h4>
        </figcaption>
</figure>

<h2 id="4-gaussian-processes">4. Gaussian Processes</h2>
<p>Ok, now we have enough information to get started with Gaussian processes. GPs are used to define a prior distribution of the functions that could explain our data. And conditional on the data we have observed we can find a posterior distribution of functions that fit the data. Let&rsquo;s say we have some <strong>known function outputs $f$</strong> and we want to infer <strong>new unknown data points $f_*$</strong>. With the kernel we&rsquo;ve described above, we can define the joint distribution $p(f, f_*)$.</p>
<div class="formula-wrap">
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
    <mo>(</mo>
    <mtable rowspacing="4pt" columnspacing="1em">
      <mtr>
        <mtd>
          <mi>f</mi>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <msub>
            <mi>f</mi>
            <mo>&#x2217;<!-- ∗ --></mo>
          </msub>
        </mtd>
      </mtr>
    </mtable>
    <mo>)</mo>
  </mrow>
  <mo>=</mo>
  <mrow class="MJX-TeXAtom-ORD">
    <mi class="MJX-tex-caligraphic" mathvariant="script">N</mi>
  </mrow>
  <mrow>
    <mo>(</mo>
    <mtable rowspacing="4pt" columnspacing="1em">
      <mtr>
        <mtd>
          <mrow>
            <mo>[</mo>
            <mtable rowspacing="4pt" columnspacing="1em">
              <mtr>
                <mtd>
                  <mi>&#x03BC;<!-- μ --></mi>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>&#x03BC;<!-- μ --></mi>
                    <mo>&#x2217;<!-- ∗ --></mo>
                  </msub>
                </mtd>
              </mtr>
            </mtable>
            <mo>]</mo>
          </mrow>
          <mo>,</mo>
          <mrow>
            <mo>[</mo>
            <mtable rowspacing="4pt" columnspacing="1em">
              <mtr>
                <mtd>
                  <mi>K</mi>
                </mtd>
                <mtd>
                  <msub>
                    <mi>K</mi>
                    <mo>&#x2217;<!-- ∗ --></mo>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msubsup>
                    <mi>K</mi>
                    <mo>&#x2217;<!-- ∗ --></mo>
                    <mi>T</mi>
                  </msubsup>
                </mtd>
                <mtd>
                  <msub>
                    <mi>K</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mo>&#x2217;<!-- ∗ --></mo>
                      <mo>&#x2217;<!-- ∗ --></mo>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
            </mtable>
            <mo>]</mo>
          </mrow>
        </mtd>
      </mtr>
    </mtable>
    <mo>)</mo>
  </mrow>
</math>
</div>
<p>So now we have a joint distribution, which we can fairly easily assemble for any new $x_*$ we are interested in. Assuming standardized data, $\mu$ and $\mu_*$ can be initialized as $\vec{0}$. And all the covariance matrices $K$ can be computed for all the data points we&rsquo;re interested in.</p>
<ul>
<li>$K = k(x, x)$</li>
<li>$K_* = k(x, x_*)$</li>
<li>$K_{**} = k(x_*, x_*)$</li>
</ul>
<p>Because this distribution only forces the samples to be smooth functions, there should be infinitely many functions that fit $f$.</p>
<p>And now comes the most important part. <strong>Given a prior $f_{prior}$ Gaussian, wich we assume to be the marginal distribution, we can compute the conditional distribution $f_*|f$ (as we have observed $f$).</strong>. Which is something we can calculate because it is a Gaussian. However, to do so, we need to go through some very tedious mathematics. We will take this for granted and will only work with the end result. <a href="http://www.gaussianprocess.org/gpml/chapters/">Gaussian processes for machine learning</a>, presents the algebraic steps needed to compute this
conditional probability. We&rsquo;ll end up with the two parameters need for our new probability distribution $\mu_*$ and $\Sigma_*$, giving us the distribution over functions we are interested in.</p>
<p>$$f_* = \mathcal{N}(\mu_*, \Sigma_*)$$</p>
<p>A quick note, before we&rsquo;ll dive into it. The resulting Gaussian probabilities are written in term of a unit Gaussian. Both of the next distributions are equal.</p>
<p>$$\mathcal{N}(\mu, \sigma) = \mu + \sigma \mathcal{N}(0, 1) $$</p>
<h3 id="setup">Setup</h3>
<p>Next part of the post we&rsquo;ll derive posterior distribution for a <strong>GP</strong>. Therefore we&rsquo;ll need some test data. Let&rsquo;s assume a true function $f = sin(x)$ from which we have observed 5 data points.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">512</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># True function</span>
</span></span><span style="display:flex;"><span>f <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> x: np<span style="color:#f92672">.</span>sin(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Known domain of true function f</span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">6</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(x, f(x))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlim(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">10</span>)
</span></span></code></pre></div><figure><img src="../../../../../img/post-21-gp/datapoints.png"/><figcaption>
            <h4>Observed values of $f$</h4>
        </figcaption>
</figure>

<h3 id="prior-probability">Prior probability</h3>
<p>We first set up the new domain $x_{*}$ (i.e. the features we want to predict) and apply the kernel $k_{**} = k(x_{*}, x_{*})$. This results in our new covariance matrix for our prior distribution. Instead of parameterizing our prior with this covariance matrix, we take the <a href="https://en.wikipedia.org/wiki/Cholesky_decomposition">Cholesky decomposition</a> $\text{cholesky}(k_{**})$, which in this context can be seen a square root operation for matrices and thus transforming the variance into the standard deviation. As we
assume standardized data ($\mu = 0$), we can ignore $\mu_{*}$.</p>
<p>$$ p(f_{*}) = \text{cholesky}(k_{**}) \mathcal{N}(0, I) $$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>n <span style="color:#f92672">=</span> <span style="color:#ae81ff">200</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Domain of f* we want to infer</span>
</span></span><span style="display:flex;"><span>x_s <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">10</span>, n)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># covariance matrix</span>
</span></span><span style="display:flex;"><span>K_ss <span style="color:#f92672">=</span> kernel(x_s, x_s, l<span style="color:#f92672">=</span><span style="color:#ae81ff">0.8</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Is square root of matrix (standard deviation)</span>
</span></span><span style="display:flex;"><span>L_ss <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>cholesky(K_ss <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-6</span> <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>eye(n))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># N~(0, I) * L</span>
</span></span><span style="display:flex;"><span>f_prior <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(L_ss, np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(size<span style="color:#f92672">=</span>(n, <span style="color:#ae81ff">15</span>)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">6</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(x_s, f_prior)
</span></span></code></pre></div><figure><img src="../../../../../img/post-21-gp/prior.png"/><figcaption>
            <h4>Samples from the prior distribution of $f_*$</h4>
        </figcaption>
</figure>

<h3 id="posterior-probability">Posterior probability</h3>
<p><strong>mean</strong></p>
<p>Now we will find the mean and covariance matrix for the posterior. Let&rsquo;s start with the mean $\mu_*$.</p>
<p>$$ \mu_* =  k_*^T \alpha $$</p>
<p>Where $\alpha = (L^T)^{-1} \cdot L^{-1}f$, $L = \text{cholesky}(k + \sigma_n^2 I)$, and $\sigma_n^2$ is the noise in the observations (can be close to zero for noise-less regression).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># find mean mu*</span>
</span></span><span style="display:flex;"><span>K <span style="color:#f92672">=</span> kernel(x, x, l<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>)
</span></span><span style="display:flex;"><span>K_s <span style="color:#f92672">=</span> kernel(x, x_s, l<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>)
</span></span><span style="display:flex;"><span>L <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>cholesky(K <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-6</span> <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>eye(len(x)))
</span></span><span style="display:flex;"><span>alpha <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>solve(L<span style="color:#f92672">.</span>T, np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>solve(L, f(x)))
</span></span><span style="display:flex;"><span>mu <span style="color:#f92672">=</span> K_s<span style="color:#f92672">.</span>T <span style="color:#f92672">@</span> alpha
</span></span></code></pre></div><p><strong>covariance</strong></p>
<p>The covariance matrix is defined by</p>
<p>$$\Sigma_* = k_{**} - v^Tv$$</p>
<p>where $v = L^{-1} k_*$.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># determine covariance matrix</span>
</span></span><span style="display:flex;"><span>v <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>solve(L, K_s)
</span></span><span style="display:flex;"><span>variance <span style="color:#f92672">=</span> K_ss <span style="color:#f92672">-</span> v<span style="color:#f92672">.</span>T <span style="color:#f92672">@</span> v
</span></span></code></pre></div><p>Let $B = \text{cholesky}(\Sigma_* + \sigma_n^2 I)$ and we can sample from the posterior by</p>
<p>$$ p(f_*|f) = \mu_* + B \mathcal{N}(0, I)$$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Sample 10 functions from the posterior</span>
</span></span><span style="display:flex;"><span>B <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>cholesky(variance <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-6</span> <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>eye(variance<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]))
</span></span><span style="display:flex;"><span>f_post <span style="color:#f92672">=</span> mu[:, <span style="color:#66d9ef">None</span>] <span style="color:#f92672">+</span> B <span style="color:#f92672">@</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(size<span style="color:#f92672">=</span>(n, <span style="color:#ae81ff">10</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Find standard deviation for plotting uncertainty values.</span>
</span></span><span style="display:flex;"><span>s2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones(n) <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>sum(v<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>std <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sqrt(s2)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">6</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(x, f(x), <span style="color:#e6db74">&#39;bs&#39;</span>, ms<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(x_s, f_post)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>fill_between(x_s, mu <span style="color:#f92672">-</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> std, mu <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> std, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;#dddddd&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(x_s, mu, <span style="color:#e6db74">&#39;r--&#39;</span>, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><figure><img src="../../../../../img/post-21-gp/posterior.png"/><figcaption>
            <h4>10 samples from the posterior</h4>
        </figcaption>
</figure>

<p>In the plot above we see the result from our posterior distribution. We sample functions that fit our training data (the red squares). So the amount of possible infinite functions that could describe our data has been reduced to a lower amount of infinite functions [if that makes sense ;)]. It is also very nice that we get uncertainty boundaries are smaller in places where we have observed data and widen where we have not. For now, we did noiseless regressions, so the
uncertainty is nonexistent where we observed data. We could generalize this example to noisy data and also include functions that are within the noise margin. The red dashed line shows the mean of the posterior and would now be our best guess for $f(x)$.</p>
<p>This post was an introduction to Gaussian processes and described what it meant to express functions as samples from a distribution. I hope it gave some insight into the abstract definition of <strong>GPs</strong>. You may also take a look at <a href="https://www.ritchievink.com/blog/2018/06/05/clustering-data-with-dirichlet-mixtures-in-edward-and-pymc3/">Gaussian mixture models</a> where we utilize Gaussian and Dirichlet distributions to do nonparametric clustering.</p>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
  </script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<head>
<style>

.formula-wrap {
overflow-x: scroll;
}

</style>
</head>

    </div>
    
    
  </div>
</section>

<section class="section">
  <div class="container">
    <aside><div id="disqus_thread"></div></aside>
    <script type="text/javascript">
      var disqus_shortname = 'www-ritchievink-com';
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
  </div>
</section>


<section class="section">
  <div class="container has-text-centered">
    <p>(c) 2020 Ritchie Vink.</p>
  </div>
</section>

<script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.0/build/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/javascript">
    if (window.location.href.indexOf('localhost') < 0) {
	    var _gaq = _gaq || [];
	    _gaq.push(['_setAccount', 'UA-83196691-2']);
	    _gaq.push(['_trackPageview']);

	    (function() {
		var ga = document.createElement('script');
		ga.src = ('https:' == document.location.protocol ? 'https://ssl' :
		    'http://www') + '.google-analytics.com/ga.js';
		ga.setAttribute('async', 'true');
		document.documentElement.firstChild.appendChild(ga);
	    })();
}
</script>




</body>

<!DOCTYPE html>
<html lang="en-EN">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta content="keyword 1, keyword 2, keyword 3" name="keywords">
<meta content="Ritchie Vink" name="author">
<meta property="og:title" content="Distribution estimation with Masked Autoencoders - Ritchie Vink">
<meta property="og:url" content="https://www.ritchievink.com/blog/2019/10/25/distribution-estimation-with-masked-autoencoders/">
<meta property="og:description" content="">
<meta property="og:type" content="website" />


<meta property="og:image" content="https://www.ritchievink.com/img/post-29-made/og_image.jpg" />


<title>Distribution estimation with Masked Autoencoders | Ritchie Vink</title>

<link rel="stylesheet" href="https://www.ritchievink.com//css/style.css">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />

<link rel="stylesheet"
      href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.0/build/styles/default.min.css">

</head>

<body>
<section class="section">
  <div class="container">
    <nav class="nav">

 <img src="../../../../../profile.jpg" alt="Avatar" style="margin-right: 1em" height="100px"> 
      <div class="nav-left" style="flex-basis: auto;">

        <a class="nav-item" href="https://www.ritchievink.com/"><h1 class="title is-4">Ritchie Vink</h1></a>
      <nav class="nav-item level is-mobile">
          <a class="level-item" href="../../../../../tags">
            tags
          </a>
          
          
          <a class="level-item" href="https://www.ritchievink.com/about/">
            about
          </a>
          
          <a class="level-item" href="https://www.ritchievink.com/anastruct/">
            anastruct
          </a>
          
        </nav>
      </div>
      <div class="nav-right">
        <nav class="nav-item level is-mobile">
          
          <a class="level-item" href="https://github.com/ritchie46" target="_blank">
            <span class="icon">
              <i class="fa fa-github"></i>
            </span>
          </a>
          
          <a class="level-item" href="https://linkedin.com/in/ritchievink/" target="_blank">
            <span class="icon">
              <i class="fa fa-linkedin-square"></i>
            </span>
          </a>
          
          <a class="level-item" href="https://www.ritchievink.com/index.xml" target="_blank">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>
          </a>
          
        </nav>
      </div>
    </nav>
  </div>
</section>

<section class="section">
  <div class="container">
    <h1 class="title">Distribution estimation with Masked Autoencoders</h1>
    <h2 class="subtitle is-5">October 25, 2019 by Ritchie Vink</h2>
    
      <div class="tags">
    
        <a class="button is-link" href="https://www.ritchievink.com/tags/machine-learning">machine learning</a>
    
        <a class="button is-link" href="https://www.ritchievink.com/tags/python">python</a>
    
        <a class="button is-link" href="https://www.ritchievink.com/tags/deep-learning">deep-learning</a>
    
</div>

    
    <div class="content">
      <figure><img src="../../../../../img/post-29-made/og_image.jpg"/>
</figure>

<p>Four of my last five blog posts were more or less related to Baysian inference with variational methods. I had some momentum, and I wanted to use the traction I gained to do another post (which will come!) on enhancing variational methods with Inverse Autoregressive Flows (IAF), but first I have to get something different out of the way.</p>
<p>In the paper describing <a href="https://arxiv.org/abs/1606.04934">IAF</a>, they refer to an autoregressive neural network (and further assume his to be clear knowlegde). Besides that I now needed to research autoregressive neural networks in order to fully understand the IAF paper, it also triggered my interest without being a roadblock. And, as it turns out, they turn to be a quite simple, but really cool on standard autoencoders. This post we will take a look at autoregressive neural networks implemented as masked autoencoders.</p>
<h2 id="1-default-autoencoders">1. Default autoencoders</h2>
<p>Default autoencoder try to reconstruct their input while we as algorithm designers try to prevent them from doing so (a little bit). They must a feel bit like the bullied robot in the video below. We give them a task, but also hinder the robot in doing so.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/-Wnp-OOZB34?start=10" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p>Obviously we put the autoencoders of balance by shoving them with a hockey stick. Our hindrance is more subtle. We give them a task, but we don&rsquo;t provide them the required tools for executing that task. When building a shed with only have a hammer and three nails at you disposition, you have to be really creative.</p>
<p>The figurative nails and hammer are called a bottleneck, often named $z$, and this a narrow layer in the middle of the network. A bit more formally, let $x \in \mathbb{R}^D$, then $z \in R^{&lt;D}$. The autoencoder consists of two parts. The encoder; $f(x) = z$, and the decoder $g(z) = x$.</p>
<p>The figure below shows a visual representation of an autoencoder. Where the $z$ is the latent space representation.</p>
<figure><img src="../../../../../img/post-29-made/autoencoder.png"/><figcaption>
            <h4>Autoencoder architecture [1].</h4>
        </figcaption>
</figure>

<p>By reducing the latent dimension, we enforce the autoencoder to map the input to a lower dimension, whilst retaining as much of the information a possible. This for instance help in:</p>
<ul>
<li>Denoising images (The lower dimension of $z$ filters the noise).</li>
<li>Compress data (Only call the encoder $f(x)$).</li>
</ul>
<h2 id="2-distribution-estimation">2. Distribution estimation</h2>
<p>If we bully the autoencoders just a bit more, by also blinding them partially, we can actually make them learn $P(x)$, i.e. the distribution of $x$. <a href="https://arxiv.org/abs/1502.03509">Germain, Gregor &amp; Larochelle $^{[2]}$</a>, posted their findings in the paper <strong>MADE: Masked Autoencoder for Density Estimation</strong>.
In my opion, they made a really elegant observation that, by the definition of the chain rule of probability, we can learn $P(x)$ by blinding (masking) an autoencoder. Let&rsquo;s explore their observation.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Chain_rule_(probability)">chain rule of probability</a> states:</p>
<p>\begin{eqnarray}
P(A,B) = P(A|B) \cdot P(B)
\end{eqnarray}</p>
<p>Which for a whole vector of $x$ is defined by:</p>
<div>
\begin{eqnarray}
P(x) &=& P(x_1) \cdot P(x_2| x_1) \cdot P(x_D | x_{1:D-1}) \\
  &=& \prod_{d=1}^{D}P(x_d | x_{\lt d} )
\end{eqnarray}
</div>
<p>So by learning an autoregressive relationship, we actually model the probability distribution of $x$. That&rsquo;s really cool, and actually quite simple to implement! This is where the bullying of the autoencoder comes into play. All we need to do is to restrict autoencoders connections in such a way that node for predicting $x_t$, is only connected to the inputs $x_{1:t-1}$.</p>
<p>This principle of restricted connections is shown in the figure below for the red output $P(x_2|x_1)$. Please take a look good at it and appreciate what you see, because it took me forever to draw!</p>
<figure><img src="../../../../../img/post-29-made/autoregessive-weights.png"/><figcaption>
            <h4>Autoregressive autoencoder [3].</h4>
        </figcaption>
</figure>

<h2 id="3-masking-trick">3. Masking trick</h2>
<p>The autoregressive properties of such a network are obvious, however the implementation doesn&rsquo;t seem trivial at first sight. In the figure, the autoencoder is drawn with reduced connections. This isn&rsquo;t how we are going to implement it however. Just as done with dropout$^{[4]}$, we nullify weight outputs by element-wise multiplying them with a binary masking matrix. Connections multiplied by one are unharmed. Connections multiplied by zero are effectually discarded.
A standard neural networks layer is defined by $g(Wx + b)$, where $W$ is a weight matrix, $b$ is a bias vector, and $g$ is a non linear function. With masked autoencoders, the layer activation will become $g((W \odot M)x + b)$, where $M \in \{0, 1\}$ is the masking matrix.</p>
<h3 id="31-hidden-nodes">3.1 Hidden nodes</h3>
<p>For the creation of the masks, we use a clever trick. We assign a random variable $m&rsquo; \in \{1, 2, \dots, D-1 \}$ to every hidden node $k$ in the network.</p>
<p>Let the value of $m&rsquo;$ at node $k$ and layer $l$ be $m^l(k) = m&rsquo;_k $.
The masking values $M_{k, k&rsquo;}$ for node $k&rsquo;$ connected to node $k$ (of the previous layer) are defined by:</p>
<div>
\begin{eqnarray}
M_{k, k'} = 1_{m^l(k') \ge m^{l-1}(k)} \label{eq:hidden} \\
\end{eqnarray}
</div>
<p>Where $1_{m^l(k&rsquo;) \ge m^{l-1}(k)}$ is the indicator function, returning $1$ if the expression is true and retuning $0$ otherwise.</p>
<h2 id="32-output-nodes">3.2 Output nodes</h2>
<p>For the output nodes of the neural network, the condition slightly changes. Let $d$ be the ouput node. The masking values are then defined by:</p>
<div>
\begin{eqnarray}
M_{d, k'} = 1_{m^l(d) \gt m^{l-1}(k)} \label{eq:out} \\
\end{eqnarray}
</div>
<p>Note that $\ge$ becomes $\gt$. This is important as we need to shift the connections by one. The first output $x_1$ may not be connected to any nodes as it is not conditioned by any inputs.</p>
<h3 id="33-example">3.3 Example</h3>
<p>The figure below shows an example of the masks that would be generated by this algorithm. The connections of the blue (hidden) nodes are determined by eq. $\ref{eq:hidden}$. The connections of the output nodes are determined by eq. $\ref{eq:out}$.</p>
<figure><img src="../../../../../img/post-29-made/masks-example.png"/><figcaption>
            <h4>Example of masking algorithm [3].</h4>
        </figcaption>
</figure>

<p>The first output $P(x_1)$ is not conditioned on anything and nothing is conditioned on the last output $P(x_D)$, hence $d=4$ of the input and $d=1$ of the output don&rsquo;t have any connections.</p>
<h2 id="4-implementation-autoregressive-neural-network">4. Implementation Autoregressive Neural Network</h2>
<p>That&rsquo;s a wrap for the theoretical part. We now have got all that is required to implement an autoregressive neural network.In this section we will create the <strong>MADE</strong> architecture in pytorch.</p>
<h3 id="41-linear-layer">4.1 Linear layer</h3>
<p>First we start with a <code>LinearMasked</code> layer, which is a substitute for the default <code>nn.Linear</code> in pytorch.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LinearMasked</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, in_features, out_features, num_input_features, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Parameters
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        ----------
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        in_features : int
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        out_features : int
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        num_input_features : int
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            Number of features of the models input X.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            These are needed for all masked layers.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        bias : bool
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        super(LinearMasked, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>linear <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(in_features, out_features, bias)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>num_input_features <span style="color:#f92672">=</span> num_input_features
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> (
</span></span><span style="display:flex;"><span>            out_features <span style="color:#f92672">&gt;=</span> num_input_features
</span></span><span style="display:flex;"><span>        ), <span style="color:#e6db74">&#34;To ensure autoregression, the output there should be enough hidden nodes. h &gt;= in.&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Make sure that d-values are assigned to m</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># d = 1, 2, ... D-1</span>
</span></span><span style="display:flex;"><span>        d <span style="color:#f92672">=</span> set(range(<span style="color:#ae81ff">1</span>, num_input_features))
</span></span><span style="display:flex;"><span>	c <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>	    c <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>	    <span style="color:#66d9ef">if</span> c <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">10</span>:
</span></span><span style="display:flex;"><span>	        <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># m function of the paper. Every hidden node, gets a number between 1 and D-1</span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>m <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">1</span>, num_input_features, size<span style="color:#f92672">=</span>(out_features,))<span style="color:#f92672">.</span>type(
</span></span><span style="display:flex;"><span>                torch<span style="color:#f92672">.</span>int32
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> len(d <span style="color:#f92672">-</span> set(self<span style="color:#f92672">.</span>m<span style="color:#f92672">.</span>numpy())) <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>register_buffer(
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;mask&#34;</span>, torch<span style="color:#f92672">.</span>ones_like(self<span style="color:#f92672">.</span>linear<span style="color:#f92672">.</span>weight)<span style="color:#f92672">.</span>type(torch<span style="color:#f92672">.</span>uint8)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">set_mask</span>(self, m_previous_layer):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Sets mask matrix of the current layer.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Parameters
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        ----------
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        m_previous_layer : tensor
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            m values for previous layer layer.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            The first layers should be incremental except for the last value,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            as the model does not make a prediction P(x_D+1 | x_&lt;D + 1).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            The last prediction is P(x_D| x_&lt;D)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>mask[<span style="color:#f92672">...</span>] <span style="color:#f92672">=</span> (m_previous_layer[:, <span style="color:#66d9ef">None</span>] <span style="color:#f92672">&lt;=</span> self<span style="color:#f92672">.</span>m[<span style="color:#66d9ef">None</span>, :])<span style="color:#f92672">.</span>T
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>linear<span style="color:#f92672">.</span>bias <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            b <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            b <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>linear<span style="color:#f92672">.</span>bias
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> F<span style="color:#f92672">.</span>linear(x, self<span style="color:#f92672">.</span>linear<span style="color:#f92672">.</span>weight <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>mask, b)
</span></span></code></pre></div><p>In the <code>__init__</code> method we create the values $m&rsquo;$ discussed in section 2, and assign those to every node in the layer. The <code>while</code> loop, is to ensure that every unique value $d \in \{1, 2, \dots, D-1 \}$ is at least assigned once.</p>
<h3 id="42-sequential-utility">4.2 Sequential utility</h3>
<p>Next we create a substitute for pytorch&rsquo; <code>nn.Sequential</code> utility. The reason for this replacement is that every subsequent <code>LinearMasked</code> layer sets the mask dependent of the $m&rsquo;$ values of the layer above. The <code>SequentialMasked</code> will call the <code>set_mask</code> method with the proper values for $m&rsquo;$.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SequentialMasked</span>(nn<span style="color:#f92672">.</span>Sequential):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, <span style="color:#f92672">*</span>args):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__(<span style="color:#f92672">*</span>args)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        input_set <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(args)):
</span></span><span style="display:flex;"><span>            layer <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>__getitem__(i)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> isinstance(layer, LinearMasked):
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">continue</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> input_set:
</span></span><span style="display:flex;"><span>                layer <span style="color:#f92672">=</span> set_mask_input_layer(layer)
</span></span><span style="display:flex;"><span>                m_previous_layer <span style="color:#f92672">=</span> layer<span style="color:#f92672">.</span>m
</span></span><span style="display:flex;"><span>                input_set <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                layer<span style="color:#f92672">.</span>set_mask(m_previous_layer)
</span></span><span style="display:flex;"><span>                m_previous_layer <span style="color:#f92672">=</span> layer<span style="color:#f92672">.</span>m
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">set_mask_last_layer</span>(self):
</span></span><span style="display:flex;"><span>        reversed_layers <span style="color:#f92672">=</span> filter(
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">lambda</span> l: isinstance(l, LinearMasked), reversed(self<span style="color:#f92672">.</span>_modules<span style="color:#f92672">.</span>values())
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Get last masked layer</span>
</span></span><span style="display:flex;"><span>        layer <span style="color:#f92672">=</span> next(reversed_layers)
</span></span><span style="display:flex;"><span>        prev_layer <span style="color:#f92672">=</span> next(reversed_layers)
</span></span><span style="display:flex;"><span>        set_mask_output_layer(layer, prev_layer<span style="color:#f92672">.</span>m)
</span></span></code></pre></div><p>Note that the <code>SequantialMask</code> class calls two functions we don&rsquo;t have yet defined; <code>set_mask_input_layer</code> and <code>set_mask_output_layer</code>. The code snippet below makes the <code>SequentialMasked</code> complete.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">set_mask_output_layer</span>(layer, m_previous_layer):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Output layer has different m-values.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># The connection is shifted one value to the right.</span>
</span></span><span style="display:flex;"><span>    layer<span style="color:#f92672">.</span>m <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, layer<span style="color:#f92672">.</span>num_input_features)
</span></span><span style="display:flex;"><span>    layer<span style="color:#f92672">.</span>set_mask(m_previous_layer)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> layer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">set_mask_input_layer</span>(layer):
</span></span><span style="display:flex;"><span>    m_input_layer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">1</span>, layer<span style="color:#f92672">.</span>num_input_features <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    m_input_layer[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e9</span>
</span></span><span style="display:flex;"><span>    layer<span style="color:#f92672">.</span>set_mask(m_input_layer)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> layer
</span></span></code></pre></div><h3 id="43-made">4.3 MADE</h3>
<p>That&rsquo;s all that is required for the MADE model. Shown below is the final implementation of the model. Note that we don&rsquo;t use ReLU activations. A $\text{ReLU}(Wx + b) = \max(0, Wx + b)$, leading to nullified connections. This could break the autoregressive part by leaving no path from output $d$ to inputs $x_{&lt;d}$.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MADE</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Don&#39;t use ReLU, so that neurons don&#39;t get nullified.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># This makes sure that the autoregressive test can verified</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, in_features, hidden_features):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>layers <span style="color:#f92672">=</span> SequentialMasked(
</span></span><span style="display:flex;"><span>            LinearMasked(in_features, hidden_features, in_features),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ELU(),
</span></span><span style="display:flex;"><span>            LinearMasked(hidden_features, hidden_features, in_features),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ELU(),
</span></span><span style="display:flex;"><span>            LinearMasked(hidden_features, in_features, in_features),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Sigmoid(),
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>set_mask_last_layer()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>layers(x)
</span></span></code></pre></div><h3 id="44-autoregressive-validation">4.4 Autoregressive validation.</h3>
<p>We can use pytorch&rsquo; autograd to verify the autoregressive properties of the model we&rsquo;ve just defined$^{[5]}$. Below we&rsquo;ll initialize the model and feed it a tensor $x$ filled with ones. For every index in the output tensor $\hat{x}$ we compute the the partial derivate $\frac{\partial{ \hat{x} }}{ \partial{x}} $.</p>
<p><span> $\frac{\partial{ \hat{x} }_d }{ \partial{x_{ \lt d} }} $</span> should be non-zero and $\frac{\partial{ \hat{x} }_d }{ \partial{x_{\gt d} }} $ should be zero valued.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>input_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ones((<span style="color:#ae81ff">1</span>, input_size))
</span></span><span style="display:flex;"><span>x<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m <span style="color:#f92672">=</span> MADE(in_features<span style="color:#f92672">=</span>input_size, hidden_features<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> d <span style="color:#f92672">in</span> range(input_size):
</span></span><span style="display:flex;"><span>    x_hat <span style="color:#f92672">=</span> m(x)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># loss w.r.t. P(x_d | x_&lt;d)</span>
</span></span><span style="display:flex;"><span>    loss <span style="color:#f92672">=</span> x_hat[<span style="color:#ae81ff">0</span>, d]
</span></span><span style="display:flex;"><span>    loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> torch<span style="color:#f92672">.</span>all(x<span style="color:#f92672">.</span>grad[<span style="color:#ae81ff">0</span>, :d] <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> torch<span style="color:#f92672">.</span>all(x<span style="color:#f92672">.</span>grad[<span style="color:#ae81ff">0</span>, d:] <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>)
</span></span></code></pre></div><h2 id="evaluation">Evaluation</h2>
<p>I&rsquo;ve tested the model (defined in the previous section) on the Olivetti faces dataset. This dataset contains 400 64x64 images with close up faces of 40 different persons. I did not do any hyperparameter search. The hidden layer sizes were arbitrarily set on 256 nodes. The model was trained on 380 images and inference was done on the remaining 20 images.</p>
<p>At inference time I&rsquo;ve tried to predict $P(x|x_{1:\frac{D}{2}})$. The model should finish the remaining part of the image conditioned on the first part. The figure below shows a few of the results.</p>
<figure><img src="../../../../../img/post-29-made/montage.png"/><figcaption>
            <h4>Results of the MADE model conditioned on the first half of the images.</h4>
        </figcaption>
</figure>

<h2 id="last-words">Last words</h2>
<p>Once you see it, it looks so simple. That&rsquo;s probably the case with most good ideas. I really like the simplicity of this architecture. We were able to create a generative model (learning $P(x)$, just by applying masks to an autoencoder. The research made for the MADE paper, formed the basis for even more generative models as PixelRNN/ CNN (creating images pixel for pixel) and even the very cool wavenet (speech synthesis). I must mention that inference with this model architecture is super slow. This is due to the autoregressive properties of the model. During training, this isn&rsquo;t an issue as all data points in the future $x_{t+n}$
are already known. At inference time, we need to predict them one by one, without any parallelization. This slow inference time is the reason why next post won&rsquo;t be about Autoregressive flows. Instead it will be about <strong>Inverse</strong> Autoregressive flows and we will use the the autoregressive we have discussed today. But first, weekend!</p>
<h2 id="references">References</h2>
<p>  [1] Brendan Fortuner (2018, Aug 11) <em>Machine Learning Glossary</em> Retrieved from <a href="https://github.com/bfortuner/ml-cheatsheet">https://github.com/bfortuner/ml-cheatsheet</a> <br>
  [2] Germain, Gregor &amp; Larochelle (2015, Feb 12) <em>MADE: Masked Autoencoder for Distribution Estimation</em>. Retrieved from <a href="https://arxiv.org/abs/1502.03509">https://arxiv.org/abs/1502.03509</a> <br>
  [3]  NPTEL-NOC IITM (2019, Apr 19) <em>Deep Learning Part - II (CS7015): Lec 21.2 Masked Autoencoder Density Estimator (MADE)</em>. Retrieved from <a href="https://youtu.be/lNW8T0W-xeE">https://youtu.be/lNW8T0W-xeE</a> <br>
  [4] Hinton et al. (2012, Jul 3) <em>Improving neural networks by preventing co-adaptation of feature detectors</em>. Retrieved from <a href="https://arxiv.org/abs/1207.0580">https://arxiv.org/abs/1207.0580</a><br>
  [5] Karpathy, A (2018, Apr 22) <em>pytorch-made</em>. Retrieved from <a href="https://github.com/karpathy/pytorch-made">https://github.com/karpathy/pytorch-made</a> <br></p>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
  </script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<head>
<style>

.formula-wrap {
overflow-x: scroll;
}

</style>
</head>

    </div>
    
    
  </div>
</section>

<section class="section">
  <div class="container">
    <aside><div id="disqus_thread"></div></aside>
    <script type="text/javascript">
      var disqus_shortname = 'www-ritchievink-com';
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
  </div>
</section>


<section class="section">
  <div class="container has-text-centered">
    <p>(c) 2020 Ritchie Vink.</p>
  </div>
</section>

<script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.0/build/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



<script type="text/javascript">
    if (window.location.href.indexOf('localhost') < 0) {
	    var _gaq = _gaq || [];
	    _gaq.push(['_setAccount', 'UA-83196691-2']);
	    _gaq.push(['_trackPageview']);

	    (function() {
		var ga = document.createElement('script');
		ga.src = ('https:' == document.location.protocol ? 'https://ssl' :
		    'http://www') + '.google-analytics.com/ga.js';
		ga.setAttribute('async', 'true');
		document.documentElement.firstChild.appendChild(ga);
	    })();
}
</script>




</body>

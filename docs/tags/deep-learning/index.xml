<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep learning on Ritchie Vink</title>
    <link>https://www.ritchievink.com/tags/deep-learning/</link>
    <description>Recent content in deep learning on Ritchie Vink</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-EN</language>
    <managingEditor>ritchie46@gmail.com (Ritchie Vink)</managingEditor>
    <webMaster>ritchie46@gmail.com (Ritchie Vink)</webMaster>
    <copyright>(c) 2020 Ritchie Vink.</copyright>
    <lastBuildDate>Tue, 12 Nov 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://www.ritchievink.com/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Another normalizing flow: Inverse Autoregressive Flows</title>
      <link>https://www.ritchievink.com/blog/2019/11/12/another-normalizing-flow-inverse-autoregressive-flows/</link>
      <pubDate>Tue, 12 Nov 2019 00:00:00 +0000</pubDate>
      <author>ritchie46@gmail.com (Ritchie Vink)</author>
      <guid>https://www.ritchievink.com/blog/2019/11/12/another-normalizing-flow-inverse-autoregressive-flows/</guid>
      <description>This post we will explore a type of normalizing flow called **Inverse Autoregressive Flow**. A composition (flow) of transformations, while preserving the constraints of a probability distribution (normalizing), can help us obtain highly correlated variational distributions. Don&amp;rsquo;t repeat yourself
If what was mentioned in the previous lines didn&amp;rsquo;t ring a bell, do first read these posts: variational inference and normalizing flows. This post could really be seen as an extension of the latter.</description>
    </item>
    
    <item>
      <title>Distribution estimation with Masked Autoencoders</title>
      <link>https://www.ritchievink.com/blog/2019/10/25/distribution-estimation-with-masked-autoencoders/</link>
      <pubDate>Fri, 25 Oct 2019 00:00:00 +0000</pubDate>
      <author>ritchie46@gmail.com (Ritchie Vink)</author>
      <guid>https://www.ritchievink.com/blog/2019/10/25/distribution-estimation-with-masked-autoencoders/</guid>
      <description>Four of my last five blog posts were more or less related to Baysian inference with variational methods. I had some momentum, and I wanted to use the traction I gained to do another post (which will come!) on enhancing variational methods with Inverse Autoregressive Flows (IAF), but first I have to get something different out of the way.
In the paper describing IAF, they refer to an autoregressive neural network (and further assume his to be clear knowlegde).</description>
    </item>
    
    <item>
      <title>Generative Adversarial Networks in Pytorch: The distribution of Art</title>
      <link>https://www.ritchievink.com/blog/2018/07/16/generative-adversarial-networks-in-pytorch-the-distribution-of-art/</link>
      <pubDate>Mon, 16 Jul 2018 00:00:00 +0000</pubDate>
      <author>ritchie46@gmail.com (Ritchie Vink)</author>
      <guid>https://www.ritchievink.com/blog/2018/07/16/generative-adversarial-networks-in-pytorch-the-distribution-of-art/</guid>
      <description>Generative adversarial networks seem to be able to generate amazing stuff. I wanted to do a small project with GANs and in the process create something fancy for on the wall. Therefore I tried to train a GAN on a dataset of art paintings. This post I&amp;rsquo;ll explore if I&amp;rsquo;ll succeed in getting a full hd new Picasso on the wall. The pictures above give you a glimplse of some of the results from the model.</description>
    </item>
    
    <item>
      <title>Transfer learning with Pytorch: Assessing road safety with computer vision</title>
      <link>https://www.ritchievink.com/blog/2018/04/12/transfer-learning-with-pytorch-assessing-road-safety-with-computer-vision/</link>
      <pubDate>Thu, 12 Apr 2018 00:00:00 +0000</pubDate>
      <author>ritchie46@gmail.com (Ritchie Vink)</author>
      <guid>https://www.ritchievink.com/blog/2018/04/12/transfer-learning-with-pytorch-assessing-road-safety-with-computer-vision/</guid>
      <description>For a project at Xomnia, I had the oppertunity to do a cool computer vision assignment. We tried to predict the input of a road safety model. Eurorap is such a model. In short, it works something like this. You take some cars, mount them with cameras and drive around the road you&amp;rsquo;re interested in. The &amp;lsquo;Google Streetview&amp;rsquo; like material you&amp;rsquo;ve collected is sent to a crowdsourced workforce (at Amazon they are called Mechanical Turks) to manually label the footage.</description>
    </item>
    
    <item>
      <title>Programming a neural network from scratch</title>
      <link>https://www.ritchievink.com/blog/2017/07/10/programming-a-neural-network-from-scratch/</link>
      <pubDate>Mon, 10 Jul 2017 12:11:12 +0200</pubDate>
      <author>ritchie46@gmail.com (Ritchie Vink)</author>
      <guid>https://www.ritchievink.com/blog/2017/07/10/programming-a-neural-network-from-scratch/</guid>
      <description>Intro At the moment of writing this post it has been a few months since I&amp;rsquo;ve lost myself in the concept of machine learning. I have been using packages like TensorFlow, Keras and Scikit-learn to build a high conceptual understanding of the subject. I did understand intuitively what the backpropagation algorithm and the idea of minimizing costs does, but I hadn&amp;rsquo;t programmed it myself. Tensorflow is regarded as quite a low level machine learning package, but it still abstracts the backpropagation algorithm for you.</description>
    </item>
    
    <item>
      <title>Deep learning music classifier part 2. Computer says no!</title>
      <link>https://www.ritchievink.com/blog/2017/06/04/deep-learning-music-classifier-part-2.-computer-says-no/</link>
      <pubDate>Sun, 04 Jun 2017 14:08:34 +0200</pubDate>
      <author>ritchie46@gmail.com (Ritchie Vink)</author>
      <guid>https://www.ritchievink.com/blog/2017/06/04/deep-learning-music-classifier-part-2.-computer-says-no/</guid>
      <description>Recap Last post I described what my motivations were to start building a music classifier, or at least attempt to build one. The post also described how I collected a dataset, extracted important features and clustered the data based on their variance. You can read the previous post here.
This post describes how I got my feet wet with classifying music. Spotify kind of sorted the data I&amp;rsquo;ve downloaded by genre.</description>
    </item>
    
    <item>
      <title>Deep learning music classifier part 1. 30 seconds disco!</title>
      <link>https://www.ritchievink.com/blog/2017/05/12/deep-learning-music-classifier-part-1.-30-seconds-disco/</link>
      <pubDate>Fri, 12 May 2017 16:34:27 +0200</pubDate>
      <author>ritchie46@gmail.com (Ritchie Vink)</author>
      <guid>https://www.ritchievink.com/blog/2017/05/12/deep-learning-music-classifier-part-1.-30-seconds-disco/</guid>
      <description>Introduction As a nerd I am fascinated by the deep learning hype. Out of interest I have been following some courses, reading blogs and watched youtube video&amp;rsquo;s about the topic. Before diving into the content, I really thought this was something solely for the great internet companies and that it was not a subject us mortals could understand.
While reading and learning more about it I&amp;rsquo;ve come to the insight that making use of deep learning techniques is not only something the internet giants and scientists can do.</description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>algorithm breakdown on Ritchie Vink</title>
    <link>https://www.ritchievink.com/tags/algorithm-breakdown/</link>
    <description>Recent content in algorithm breakdown on Ritchie Vink</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-EN</language>
    <managingEditor>ritchie46@gmail.com (Ritchie Vink)</managingEditor>
    <webMaster>ritchie46@gmail.com (Ritchie Vink)</webMaster>
    <copyright>(c) 2020 Ritchie Vink.</copyright>
    <lastBuildDate>Mon, 16 Sep 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://www.ritchievink.com/tags/algorithm-breakdown/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Variational inference from scratch</title>
      <link>https://www.ritchievink.com/blog/2019/09/16/variational-inference-from-scratch/</link>
      <pubDate>Mon, 16 Sep 2019 00:00:00 +0000</pubDate>
      <author>ritchie46@gmail.com (Ritchie Vink)</author>
      <guid>https://www.ritchievink.com/blog/2019/09/16/variational-inference-from-scratch/</guid>
      <description>In the posts Expectation Maximization and Bayesian inference; How we are able to chase the Posterior, we laid the mathematical foundation of variational inference. This post we will continue on that foundation and implement variational inference in Pytorch. If you are not familiar with the basis, I&amp;rsquo;d recommend reading these posts to get you up to speed.
This post we&amp;rsquo;ll model a probablistic layer as output layer of a neural network.</description>
    </item>
    
    <item>
      <title>Algorithm Breakdown: Bayesian Optimization</title>
      <link>https://www.ritchievink.com/blog/2019/08/25/algorithm-breakdown-bayesian-optimization/</link>
      <pubDate>Sun, 25 Aug 2019 00:00:00 +0000</pubDate>
      <author>ritchie46@gmail.com (Ritchie Vink)</author>
      <guid>https://www.ritchievink.com/blog/2019/08/25/algorithm-breakdown-bayesian-optimization/</guid>
      <description>Not that long ago I wrote an introduction post on Gaussian Processes (GP&amp;rsquo;s), a regression technique where we condition a Gaussian prior distribution over functions on observed data. GP&amp;rsquo;s can model any function that is possible within a given prior distribution. And we don&amp;rsquo;t get a function $f$, we get a whole posterior distribution of functions $P(f|X)$.
This of course, sounds very cool and all, but there is no free lunch.</description>
    </item>
    
    <item>
      <title>An intuitive introduction to Gaussian processes</title>
      <link>https://www.ritchievink.com/blog/2019/02/01/an-intuitive-introduction-to-gaussian-processes/</link>
      <pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate>
      <author>ritchie46@gmail.com (Ritchie Vink)</author>
      <guid>https://www.ritchievink.com/blog/2019/02/01/an-intuitive-introduction-to-gaussian-processes/</guid>
      <description>Christopher Fonnesbeck did a talk about Bayesian Non-parametric Models for Data Science using PyMC3 on PyCon 2018. In this talk, he glanced over Bayes&amp;rsquo; modeling, the neat properties of Gaussian distributions and then quickly turned to the application of Gaussian Processes, a distribution over infinite functions. Wait, but what?! How does a Gaussian represent a function? I did not understand how, but the promise of what these Gaussian Processes representing a distribution over nonlinear and nonparametric functions really intrigued me and therefore turned into a new subject for a post.</description>
    </item>
    
    <item>
      <title>Algorithm breakdown: Why do we call it Gradient Boosting?</title>
      <link>https://www.ritchievink.com/blog/2018/11/19/algorithm-breakdown-why-do-we-call-it-gradient-boosting/</link>
      <pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate>
      <author>ritchie46@gmail.com (Ritchie Vink)</author>
      <guid>https://www.ritchievink.com/blog/2018/11/19/algorithm-breakdown-why-do-we-call-it-gradient-boosting/</guid>
      <description>We were making a training at work about ensemble models. When we were discussing different techniques like bagging, boosting, and stacking, we also came on the subject of gradient boosting. Intuitively, gradient boosting, by training on the residuals made sense. However, the name gradient boosting did not right away. This post we are exploring the name of gradient boosting and of course also the model itself!
Intuition Single decision tree Gradient boosting is often used as an optimization technique for decision trees.</description>
    </item>
    
    <item>
      <title>Build Facebook&#39;s Prophet in PyMC3; Bayesian time series analyis with Generalized Additive Models</title>
      <link>https://www.ritchievink.com/blog/2018/10/09/build-facebooks-prophet-in-pymc3-bayesian-time-series-analyis-with-generalized-additive-models/</link>
      <pubDate>Tue, 09 Oct 2018 00:00:00 +0000</pubDate>
      <author>ritchie46@gmail.com (Ritchie Vink)</author>
      <guid>https://www.ritchievink.com/blog/2018/10/09/build-facebooks-prophet-in-pymc3-bayesian-time-series-analyis-with-generalized-additive-models/</guid>
      <description>Last [Algorithm Breakdown](https://www.ritchievink.com/blog/2018/09/26/algorithm-breakdown-ar-ma-and-arima-models/) we build an ARIMA model from scratch and discussed the use cases of that kind of models. ARIMA models are great when you have got stationary data and when you want to predict a few time steps into the future. A lot of business data, being generated by human processes, have got weekly and yearly seasonalities (we for instance, seem work to less in weekends and holidays) and show peaks at certain events.</description>
    </item>
    
    <item>
      <title>Algorithm Breakdown: AR, MA and ARIMA models</title>
      <link>https://www.ritchievink.com/blog/2018/09/26/algorithm-breakdown-ar-ma-and-arima-models/</link>
      <pubDate>Wed, 26 Sep 2018 00:00:00 +0000</pubDate>
      <author>ritchie46@gmail.com (Ritchie Vink)</author>
      <guid>https://www.ritchievink.com/blog/2018/09/26/algorithm-breakdown-ar-ma-and-arima-models/</guid>
      <description>Time series are a quite unique topic within machine learning. In a lot of problems the dependent variable $y$, i.e. the thing we want to predict is dependent on very clear inputs, such as pixels of an image, words in a sentence, the properties of a persons buying behavior, etc. In time series these indepent variables are often not known. For instance, in stock markets, we don&amp;rsquo;t have a clear independent set of variables where we can fit a model on.</description>
    </item>
    
    <item>
      <title>Algorithm Breakdown: Affinity Propagation</title>
      <link>https://www.ritchievink.com/blog/2018/05/18/algorithm-breakdown-affinity-propagation/</link>
      <pubDate>Fri, 18 May 2018 00:00:00 +0000</pubDate>
      <author>ritchie46@gmail.com (Ritchie Vink)</author>
      <guid>https://www.ritchievink.com/blog/2018/05/18/algorithm-breakdown-affinity-propagation/</guid>
      <description>On a project I worked on at the ANWB (Dutch road side assistence company) we mined driving behavior data. We wanted to know how many persons were likely to drive a certain vehicle on a regular basis. Naturally k-means clustering came to mind. The k-means algorithm finds clusters with the least inertia for a given k.
A drawback is that often, k is not known. For the question about the numbers of persons driving a car, this isn&amp;rsquo;t that big of a problem as we have a good estimate of what k should be.</description>
    </item>
    
    <item>
      <title>Implementing a Support Vector Machine in Scala</title>
      <link>https://www.ritchievink.com/blog/2017/11/27/implementing-a-support-vector-machine-in-scala/</link>
      <pubDate>Mon, 27 Nov 2017 16:14:21 +0100</pubDate>
      <author>ritchie46@gmail.com (Ritchie Vink)</author>
      <guid>https://www.ritchievink.com/blog/2017/11/27/implementing-a-support-vector-machine-in-scala/</guid>
      <description>This post describes the implementation of a linear support vector machine classifier (SVM) in Scala. Scala is a functional programming language that supports functional programming to a far extend. Because I am exploring Scala at the moment and I like the challenge of functional programming, the SVM will be implemented in a functional manner. We are going to test the SVM on two classes from the Iris dataset.
Linear Support Vector Machine intuition Support Vector Machines are binary classifiers.</description>
    </item>
    
    <item>
      <title>Programming a neural network from scratch</title>
      <link>https://www.ritchievink.com/blog/2017/07/10/programming-a-neural-network-from-scratch/</link>
      <pubDate>Mon, 10 Jul 2017 12:11:12 +0200</pubDate>
      <author>ritchie46@gmail.com (Ritchie Vink)</author>
      <guid>https://www.ritchievink.com/blog/2017/07/10/programming-a-neural-network-from-scratch/</guid>
      <description>Intro At the moment of writing this post it has been a few months since I&amp;rsquo;ve lost myself in the concept of machine learning. I have been using packages like TensorFlow, Keras and Scikit-learn to build a high conceptual understanding of the subject. I did understand intuitively what the backpropagation algorithm and the idea of minimizing costs does, but I hadn&amp;rsquo;t programmed it myself. Tensorflow is regarded as quite a low level machine learning package, but it still abstracts the backpropagation algorithm for you.</description>
    </item>
    
  </channel>
</rss>
